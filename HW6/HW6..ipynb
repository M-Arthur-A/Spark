{
  "metadata": {
    "name": "HW6",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\n# searching for dataset habr_data\n\nhdfs dfs -du -h /apps/spark/warehouse/\nhdfs dfs -du -h /datasets/ "
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport pyspark.sql.functions as f\n\npath \u003d \u0027/datasets/habr_data.csv\u0027\ndf_habr \u003d spark.read\\\n               .option(\"header\", \u0027true\u0027)\\\n               .option(\"inferSchema\", \"true\")\\\n               .csv(path)\ndf_habr.printSchema()\nz.show(df_habr.limit(10))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. по данным habr_data получить таблицу с названиями топ-3 статей (по rating) для каждого автора DataFrame: [“author”, “article_title”, “position” (1, 2, 3)]\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as f\n\nwindowSpec \u003d Window.partitionBy(\"author_name\").orderBy(\"rating\")\ntop_filter \u003d f.col(\u0027position\u0027) \u003c\u003d 3\n\ndf_habr\\\n.withColumn(\"position\", f.row_number().over(windowSpec)) \\\n.select(f.col(\u0027author_name\u0027), f.col(\u0027title\u0027).alias(\u0027article_title\u0027), f.col(\u0027position\u0027))\\\n.filter(top_filter)\\\n.show(truncate\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. по данным habr_data получить топ (по встречаемости) английских слов из заголовков. \n### Возможное решение: 1) выделение слов с помощью регулярных выражений, 2) разделение на массивы слов 3) explode массивовов 4) группировка с подсчетом встречаемости"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as f\n\ndf_eng_count \u003d df_habr\\\n.select(f.col(\u0027title\u0027))\\\n.withColumn(\u0027engW\u0027, f.regexp_replace(f.regexp_replace(f.regexp_replace(\\\n                                                                        f.regexp_replace(f.col(\u0027title\u0027), \"(?![A-Za-z-]{2,}).\", \" \"),\\\n                                                                                                                                    \"[ ]{2,}\", \" \"),\\\n                                                                                                                                                    \"^[ ]+\", \"\"),\\\n                                                                                                                                                                \"(\\s+)$\", \"\"))\\\n.withColumn(\u0027engW_list\u0027, f.explode(f.split(f.col(\u0027engW\u0027), \u0027 \u0027)))\n\ndf_eng_count.show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf_eng_count\\\n.select(f.col(\u0027engW_list\u0027))\\\n.where(f.col(\u0027engW_list\u0027) !\u003d \u0027\u0027)\\\n.groupBy(\u0027engW_list\u0027)\\\n.agg(f.count(\u0027*\u0027).alias(\u0027eng_word_count\u0027))\\\n.orderBy(\u0027eng_word_count\u0027, ascending\u003dFalse)\\\n.show(truncate\u003dFalse)"
    }
  ]
}